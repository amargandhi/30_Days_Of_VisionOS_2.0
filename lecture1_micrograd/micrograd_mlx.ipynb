{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf85179-696d-47dc-a30d-70ff83f3f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5061472-040f-4311-b858-08c431a1f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067094802856\n",
      "---\n",
      "x2 0.5000001192092896\n",
      "w2 0.0\n",
      "x1 -1.5000003576278687\n",
      "w1 1.000000238418579\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "# Creating array objects\n",
    "# MLX uses mx.array() instead of torch.Tensor()\n",
    "# MLX doesn't require explicit grad tracking (no requires_grad=True)\n",
    "# By default, MLX uses float32, unlike PyTorch which defaults to float32 but we used .double() for float64 MLX does not support float64\n",
    "x1 = mx.array([2.0])\n",
    "x2 = mx.array([0.0])\n",
    "w1 = mx.array([-3.0])\n",
    "w2 = mx.array([1.0])\n",
    "b = mx.array([6.8813735870195432])\n",
    "\n",
    "# Forward pass: Constructing the computational graph\n",
    "# This is similar to PyTorch, but MLX handles grad tracking implicitly\n",
    "# MLX uses a functional approach, so operations create new arrays instead of modifying in-place\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = mx.tanh(n)\n",
    "\n",
    "# Printing the output value\n",
    "# MLX arrays can be directly converted to Python scalars using .item()\n",
    "# Unlike PyTorch, there's no need for .data.item() as MLX doesn't have a separate .data attribute\n",
    "print(o.item())\n",
    "\n",
    "# Backward pass and gradient computation\n",
    "# MLX uses a functional approach with value_and_grad for combined forward and backward passes\n",
    "# This is different from PyTorch's o.backward() method\n",
    "def forward(x1, x2, w1, w2, b):\n",
    "    n = x1*w1 + x2*w2 + b\n",
    "    return mx.tanh(n).sum()  # Sum to ensure a scalar output\n",
    "    # MLX's value_and_grad requires a scalar output, hence the .sum()\n",
    "    # This differs from PyTorch where .backward() can be called on any tensor\n",
    "\n",
    "# Creating a function that computes both value and gradients\n",
    "# argnums specifies which inputs we want gradients for (all of them in this case)\n",
    "# This replaces PyTorch's .backward() and automatic grad accumulation\n",
    "grad_func = mx.value_and_grad(forward, argnums=[0, 1, 2, 3, 4])\n",
    "\n",
    "# Compute value and gradients\n",
    "# This single line replaces separate forward and backward passes in PyTorch\n",
    "# value is the output of the forward function, grads is a tuple of gradients\n",
    "value, grads = grad_func(x1, x2, w1, w2, b)\n",
    "\n",
    "print('---')\n",
    "# Printing gradients\n",
    "# In MLX, gradients are returned as a tuple from value_and_grad\n",
    "# This differs from PyTorch where gradients are stored in .grad attributes\n",
    "# We use .item() to convert single-element arrays to Python scalars\n",
    "print('x2', grads[1].item())\n",
    "print('w2', grads[3].item())\n",
    "print('x1', grads[0].item())\n",
    "print('w1', grads[2].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf318611-2a4b-4f8a-bad5-c0a6244137ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron output: array([0.999975], dtype=float32)\n",
      "Neuron weights shape: (3,)\n",
      "Neuron bias shape: (1,)\n",
      "Input x shape: (3,)\n",
      "Target shape: (1,)\n",
      "Loss: array(6.03052e-10, dtype=float32)\n",
      "Gradients of w: array([-4.82211e-09, -7.23316e-09, 2.41105e-09], dtype=float32)\n",
      "Gradient of b: array([-2.41105e-09], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "\n",
    "class Neuron(nn.Module):\n",
    "    def __init__(self, nin):\n",
    "        super().__init__()\n",
    "        self.w = mx.random.uniform(low=-1, high=1, shape=(nin,))\n",
    "        self.b = mx.random.uniform(low=-1, high=1, shape=(1,))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        act = mx.sum(self.w * x) + self.b\n",
    "        out = mx.tanh(act)\n",
    "        return out\n",
    "\n",
    "# Loss function\n",
    "def mse_loss(pred, target):\n",
    "    return mx.mean((pred - target) ** 2)\n",
    "\n",
    "# Forward pass and loss computation\n",
    "def forward_and_loss(params, x, target):\n",
    "    w, b = params\n",
    "    act = mx.sum(w * x) + b\n",
    "    pred = mx.tanh(act)\n",
    "    loss = mse_loss(pred, target)\n",
    "    return loss\n",
    "\n",
    "# Test the Neuron\n",
    "if __name__ == \"__main__\":\n",
    "    n = Neuron(3)\n",
    "    x = mx.array([2.0, 3.0, -1.0])\n",
    "    output = n(x)\n",
    "    print(f\"Neuron output: {output}\")\n",
    "\n",
    "    target = mx.array([1.0])\n",
    "    \n",
    "    # Compute loss and gradients\n",
    "    loss_and_grad_fn = mx.value_and_grad(forward_and_loss)\n",
    "    \n",
    "    # Print detailed information about the inputs\n",
    "    print(f\"Neuron weights shape: {n.w.shape}\")\n",
    "    print(f\"Neuron bias shape: {n.b.shape}\")\n",
    "    print(f\"Input x shape: {x.shape}\")\n",
    "    print(f\"Target shape: {target.shape}\")\n",
    "    \n",
    "    try:\n",
    "        loss, grads = loss_and_grad_fn([n.w, n.b], x, target)\n",
    "        print(f\"Loss: {loss}\")\n",
    "        print(f\"Gradients of w: {grads[0]}\")\n",
    "        print(f\"Gradient of b: {grads[1]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Let's try to understand the output of loss_and_grad_fn:\")\n",
    "        result = loss_and_grad_fn([n.w, n.b], x, target)\n",
    "        print(f\"Type of result: {type(result)}\")\n",
    "        print(f\"Shape of result: {mx.shape(result)}\")\n",
    "        if isinstance(result, tuple):\n",
    "            print(f\"Number of elements in result tuple: {len(result)}\")\n",
    "            for i, elem in enumerate(result):\n",
    "                print(f\"Element {i} type: {type(elem)}\")\n",
    "                print(f\"Element {i} shape: {mx.shape(elem)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92aee2bf-47c4-467b-9641-cdbfde48201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "  Neuron output: -0.999236\n",
      "  Loss: 3.996946\n",
      "  Gradients of w: array([-0.0122099, -0.0183149, 0.00610496], dtype=float32)\n",
      "  Gradient of b: -0.006105\n",
      "  Updated weights: array([-0.447477, -0.800427, 0.977206], dtype=float32)\n",
      "  Updated bias: 0.347549\n",
      "\n",
      "Epoch 1:\n",
      "  Neuron output: -0.999222\n",
      "  Loss: 3.996889\n",
      "  Gradients of w: array([-0.0124353, -0.018653, 0.00621767], dtype=float32)\n",
      "  Gradient of b: -0.006218\n",
      "  Updated weights: array([-0.446233, -0.798561, 0.976584], dtype=float32)\n",
      "  Updated bias: 0.348170\n",
      "\n",
      "Epoch 2:\n",
      "  Neuron output: -0.999207\n",
      "  Loss: 3.996830\n",
      "  Gradients of w: array([-0.0126692, -0.0190038, 0.0063346], dtype=float32)\n",
      "  Gradient of b: -0.006335\n",
      "  Updated weights: array([-0.444966, -0.796661, 0.975951], dtype=float32)\n",
      "  Updated bias: 0.348804\n",
      "\n",
      "Epoch 3:\n",
      "  Neuron output: -0.999192\n",
      "  Loss: 3.996769\n",
      "  Gradients of w: array([-0.012912, -0.019368, 0.00645598], dtype=float32)\n",
      "  Gradient of b: -0.006456\n",
      "  Updated weights: array([-0.443675, -0.794724, 0.975305], dtype=float32)\n",
      "  Updated bias: 0.349449\n",
      "\n",
      "Epoch 4:\n",
      "  Neuron output: -0.999177\n",
      "  Loss: 3.996707\n",
      "  Gradients of w: array([-0.0131642, -0.0197463, 0.00658209], dtype=float32)\n",
      "  Gradient of b: -0.006582\n",
      "  Updated weights: array([-0.442359, -0.79275, 0.974647], dtype=float32)\n",
      "  Updated bias: 0.350108\n",
      "\n",
      "Epoch 5:\n",
      "  Neuron output: -0.999160\n",
      "  Loss: 3.996641\n",
      "  Gradients of w: array([-0.0134264, -0.0201396, 0.00671319], dtype=float32)\n",
      "  Gradient of b: -0.006713\n",
      "  Updated weights: array([-0.441016, -0.790736, 0.973975], dtype=float32)\n",
      "  Updated bias: 0.350779\n",
      "\n",
      "Epoch 6:\n",
      "  Neuron output: -0.999143\n",
      "  Loss: 3.996573\n",
      "  Gradients of w: array([-0.0136992, -0.0205488, 0.00684958], dtype=float32)\n",
      "  Gradient of b: -0.006850\n",
      "  Updated weights: array([-0.439646, -0.788681, 0.973291], dtype=float32)\n",
      "  Updated bias: 0.351464\n",
      "\n",
      "Epoch 7:\n",
      "  Neuron output: -0.999125\n",
      "  Loss: 3.996502\n",
      "  Gradients of w: array([-0.0139832, -0.0209748, 0.0069916], dtype=float32)\n",
      "  Gradient of b: -0.006992\n",
      "  Updated weights: array([-0.438248, -0.786583, 0.972591], dtype=float32)\n",
      "  Updated bias: 0.352163\n",
      "\n",
      "Epoch 8:\n",
      "  Neuron output: -0.999107\n",
      "  Loss: 3.996428\n",
      "  Gradients of w: array([-0.0142792, -0.0214188, 0.0071396], dtype=float32)\n",
      "  Gradient of b: -0.007140\n",
      "  Updated weights: array([-0.43682, -0.784441, 0.971877], dtype=float32)\n",
      "  Updated bias: 0.352877\n",
      "\n",
      "Epoch 9:\n",
      "  Neuron output: -0.999087\n",
      "  Loss: 3.996351\n",
      "  Gradients of w: array([-0.0145879, -0.0218819, 0.00729396], dtype=float32)\n",
      "  Gradient of b: -0.007294\n",
      "  Updated weights: array([-0.435361, -0.782253, 0.971148], dtype=float32)\n",
      "  Updated bias: 0.353606\n",
      "\n",
      "Epoch 10:\n",
      "  Neuron output: -0.999067\n",
      "  Loss: 3.996270\n",
      "  Gradients of w: array([-0.0149102, -0.0223653, 0.0074551], dtype=float32)\n",
      "  Gradient of b: -0.007455\n",
      "  Updated weights: array([-0.43387, -0.780017, 0.970402], dtype=float32)\n",
      "  Updated bias: 0.354352\n",
      "\n",
      "Epoch 11:\n",
      "  Neuron output: -0.999046\n",
      "  Loss: 3.996185\n",
      "  Gradients of w: array([-0.015247, -0.0228704, 0.00762348], dtype=float32)\n",
      "  Gradient of b: -0.007623\n",
      "  Updated weights: array([-0.432345, -0.77773, 0.96964], dtype=float32)\n",
      "  Updated bias: 0.355114\n",
      "\n",
      "Epoch 12:\n",
      "  Neuron output: -0.999024\n",
      "  Loss: 3.996098\n",
      "  Gradients of w: array([-0.0155991, -0.0233987, 0.00779957], dtype=float32)\n",
      "  Gradient of b: -0.007800\n",
      "  Updated weights: array([-0.430785, -0.77539, 0.96886], dtype=float32)\n",
      "  Updated bias: 0.355894\n",
      "\n",
      "Epoch 13:\n",
      "  Neuron output: -0.999001\n",
      "  Loss: 3.996005\n",
      "  Gradients of w: array([-0.0159679, -0.0239519, 0.00798395], dtype=float32)\n",
      "  Gradient of b: -0.007984\n",
      "  Updated weights: array([-0.429189, -0.772994, 0.968062], dtype=float32)\n",
      "  Updated bias: 0.356693\n",
      "\n",
      "Epoch 14:\n",
      "  Neuron output: -0.998977\n",
      "  Loss: 3.995908\n",
      "  Gradients of w: array([-0.0163544, -0.0245316, 0.0081772], dtype=float32)\n",
      "  Gradient of b: -0.008177\n",
      "  Updated weights: array([-0.427553, -0.770541, 0.967244], dtype=float32)\n",
      "  Updated bias: 0.357510\n",
      "\n",
      "Epoch 15:\n",
      "  Neuron output: -0.998951\n",
      "  Loss: 3.995807\n",
      "  Gradients of w: array([-0.0167599, -0.0251399, 0.00837995], dtype=float32)\n",
      "  Gradient of b: -0.008380\n",
      "  Updated weights: array([-0.425877, -0.768027, 0.966406], dtype=float32)\n",
      "  Updated bias: 0.358348\n",
      "\n",
      "Epoch 16:\n",
      "  Neuron output: -0.998925\n",
      "  Loss: 3.995700\n",
      "  Gradients of w: array([-0.0171859, -0.0257789, 0.00859296], dtype=float32)\n",
      "  Gradient of b: -0.008593\n",
      "  Updated weights: array([-0.424159, -0.765449, 0.965547], dtype=float32)\n",
      "  Updated bias: 0.359208\n",
      "\n",
      "Epoch 17:\n",
      "  Neuron output: -0.998897\n",
      "  Loss: 3.995588\n",
      "  Gradients of w: array([-0.017634, -0.0264509, 0.00881698], dtype=float32)\n",
      "  Gradient of b: -0.008817\n",
      "  Updated weights: array([-0.422395, -0.762804, 0.964665], dtype=float32)\n",
      "  Updated bias: 0.360089\n",
      "\n",
      "Epoch 18:\n",
      "  Neuron output: -0.998867\n",
      "  Loss: 3.995470\n",
      "  Gradients of w: array([-0.0181058, -0.0271587, 0.00905291], dtype=float32)\n",
      "  Gradient of b: -0.009053\n",
      "  Updated weights: array([-0.420585, -0.760088, 0.96376], dtype=float32)\n",
      "  Updated bias: 0.360995\n",
      "\n",
      "Epoch 19:\n",
      "  Neuron output: -0.998836\n",
      "  Loss: 3.995345\n",
      "  Gradients of w: array([-0.0186034, -0.0279051, 0.00930171], dtype=float32)\n",
      "  Gradient of b: -0.009302\n",
      "  Updated weights: array([-0.418724, -0.757298, 0.96283], dtype=float32)\n",
      "  Updated bias: 0.361925\n",
      "\n",
      "Final parameters:\n",
      "Weights: array([-0.418724, -0.757298, 0.96283], dtype=float32)\n",
      "Bias: 0.361925\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "\n",
    "class Neuron(nn.Module):\n",
    "    def __init__(self, nin):\n",
    "        super().__init__()\n",
    "        self.w = mx.random.uniform(low=-1, high=1, shape=(nin,))\n",
    "        self.b = mx.random.uniform(low=-1, high=1, shape=(1,))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        act = mx.sum(self.w * x) + self.b\n",
    "        out = mx.tanh(act)\n",
    "        return out\n",
    "\n",
    "def mse_loss(pred, target):\n",
    "    return mx.mean((pred - target) ** 2)\n",
    "\n",
    "def forward_and_loss(params, x, target):\n",
    "    w, b = params\n",
    "    act = mx.sum(w * x) + b\n",
    "    pred = mx.tanh(act)\n",
    "    loss = mse_loss(pred, target)\n",
    "    return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n = Neuron(3)\n",
    "    x = mx.array([2.0, 3.0, -1.0])\n",
    "    target = mx.array([1.0])\n",
    "    \n",
    "    loss_and_grad_fn = mx.value_and_grad(forward_and_loss)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(20):  # 20 epochs for demonstration\n",
    "        output = n(x)\n",
    "        loss, grads = loss_and_grad_fn([n.w, n.b], x, target)\n",
    "        \n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        print(f\"  Neuron output: {output.item():.6f}\")\n",
    "        print(f\"  Loss: {loss.item():.6f}\")\n",
    "        print(f\"  Gradients of w: {grads[0]}\")\n",
    "        print(f\"  Gradient of b: {grads[1].item():.6f}\")\n",
    "        \n",
    "        # Parameter update (gradient descent)\n",
    "        learning_rate = 0.1\n",
    "        n.w = n.w - learning_rate * grads[0]\n",
    "        n.b = n.b - learning_rate * grads[1]\n",
    "        \n",
    "        print(f\"  Updated weights: {n.w}\")\n",
    "        print(f\"  Updated bias: {n.b.item():.6f}\")\n",
    "        print()\n",
    "\n",
    "    print(\"Final parameters:\")\n",
    "    print(f\"Weights: {n.w}\")\n",
    "    print(f\"Bias: {n.b.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac5a9e4-8c16-4eaf-8c27-91c0e2aa7b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "  Layer output: 0.958799\n",
      "  Loss: 0.001698\n",
      "  Gradients of w: array([-0.0133003, -0.0199504, 0.00665013], dtype=float32)\n",
      "  Gradient of b: -0.006650\n",
      "  Updated weights: array([-0.222269, 0.795279, 0.454978], dtype=float32)\n",
      "  Updated bias: 0.454469\n",
      "\n",
      "Epoch 1:\n",
      "  Layer output: 0.959597\n",
      "  Loss: 0.001632\n",
      "  Gradients of w: array([-0.0127957, -0.0191935, 0.00639783], dtype=float32)\n",
      "  Gradient of b: -0.006398\n",
      "  Updated weights: array([-0.22099, 0.797199, 0.454338], dtype=float32)\n",
      "  Updated bias: 0.455109\n",
      "\n",
      "Epoch 2:\n",
      "  Layer output: 0.960350\n",
      "  Loss: 0.001572\n",
      "  Gradients of w: array([-0.012328, -0.0184919, 0.00616398], dtype=float32)\n",
      "  Gradient of b: -0.006164\n",
      "  Updated weights: array([-0.219757, 0.799048, 0.453722], dtype=float32)\n",
      "  Updated bias: 0.455725\n",
      "\n",
      "Epoch 3:\n",
      "  Layer output: 0.961062\n",
      "  Loss: 0.001516\n",
      "  Gradients of w: array([-0.0118933, -0.01784, 0.00594666], dtype=float32)\n",
      "  Gradient of b: -0.005947\n",
      "  Updated weights: array([-0.218568, 0.800832, 0.453127], dtype=float32)\n",
      "  Updated bias: 0.456320\n",
      "\n",
      "Epoch 4:\n",
      "  Layer output: 0.961737\n",
      "  Loss: 0.001464\n",
      "  Gradients of w: array([-0.0114883, -0.0172324, 0.00574415], dtype=float32)\n",
      "  Gradient of b: -0.005744\n",
      "  Updated weights: array([-0.217419, 0.802555, 0.452553], dtype=float32)\n",
      "  Updated bias: 0.456894\n",
      "\n",
      "Epoch 5:\n",
      "  Layer output: 0.962379\n",
      "  Loss: 0.001415\n",
      "  Gradients of w: array([-0.01111, -0.016665, 0.005555], dtype=float32)\n",
      "  Gradient of b: -0.005555\n",
      "  Updated weights: array([-0.216308, 0.804222, 0.451997], dtype=float32)\n",
      "  Updated bias: 0.457450\n",
      "\n",
      "Epoch 6:\n",
      "  Layer output: 0.962989\n",
      "  Loss: 0.001370\n",
      "  Gradients of w: array([-0.0107558, -0.0161337, 0.00537791], dtype=float32)\n",
      "  Gradient of b: -0.005378\n",
      "  Updated weights: array([-0.215232, 0.805835, 0.45146], dtype=float32)\n",
      "  Updated bias: 0.457987\n",
      "\n",
      "Epoch 7:\n",
      "  Layer output: 0.963570\n",
      "  Loss: 0.001327\n",
      "  Gradients of w: array([-0.0104236, -0.0156353, 0.00521178], dtype=float32)\n",
      "  Gradient of b: -0.005212\n",
      "  Updated weights: array([-0.21419, 0.807399, 0.450938], dtype=float32)\n",
      "  Updated bias: 0.458509\n",
      "\n",
      "Epoch 8:\n",
      "  Layer output: 0.964125\n",
      "  Loss: 0.001287\n",
      "  Gradients of w: array([-0.0101112, -0.0151668, 0.00505561], dtype=float32)\n",
      "  Gradient of b: -0.005056\n",
      "  Updated weights: array([-0.213179, 0.808915, 0.450433], dtype=float32)\n",
      "  Updated bias: 0.459014\n",
      "\n",
      "Epoch 9:\n",
      "  Layer output: 0.964656\n",
      "  Loss: 0.001249\n",
      "  Gradients of w: array([-0.00981706, -0.0147256, 0.00490853], dtype=float32)\n",
      "  Gradient of b: -0.004909\n",
      "  Updated weights: array([-0.212197, 0.810388, 0.449942], dtype=float32)\n",
      "  Updated bias: 0.459505\n",
      "\n",
      "Epoch 10:\n",
      "  Layer output: 0.965164\n",
      "  Loss: 0.001214\n",
      "  Gradients of w: array([-0.00953954, -0.0143093, 0.00476977], dtype=float32)\n",
      "  Gradient of b: -0.004770\n",
      "  Updated weights: array([-0.211243, 0.811819, 0.449465], dtype=float32)\n",
      "  Updated bias: 0.459982\n",
      "\n",
      "Epoch 11:\n",
      "  Layer output: 0.965650\n",
      "  Loss: 0.001180\n",
      "  Gradients of w: array([-0.00927729, -0.0139159, 0.00463864], dtype=float32)\n",
      "  Gradient of b: -0.004639\n",
      "  Updated weights: array([-0.210315, 0.81321, 0.449001], dtype=float32)\n",
      "  Updated bias: 0.460446\n",
      "\n",
      "Epoch 12:\n",
      "  Layer output: 0.966117\n",
      "  Loss: 0.001148\n",
      "  Gradients of w: array([-0.00902908, -0.0135436, 0.00451454], dtype=float32)\n",
      "  Gradient of b: -0.004515\n",
      "  Updated weights: array([-0.209413, 0.814565, 0.44855], dtype=float32)\n",
      "  Updated bias: 0.460897\n",
      "\n",
      "Epoch 13:\n",
      "  Layer output: 0.966565\n",
      "  Loss: 0.001118\n",
      "  Gradients of w: array([-0.00879379, -0.0131907, 0.00439689], dtype=float32)\n",
      "  Gradient of b: -0.004397\n",
      "  Updated weights: array([-0.208533, 0.815884, 0.44811], dtype=float32)\n",
      "  Updated bias: 0.461337\n",
      "\n",
      "Epoch 14:\n",
      "  Layer output: 0.966996\n",
      "  Loss: 0.001089\n",
      "  Gradients of w: array([-0.00857047, -0.0128557, 0.00428523], dtype=float32)\n",
      "  Gradient of b: -0.004285\n",
      "  Updated weights: array([-0.207676, 0.817169, 0.447681], dtype=float32)\n",
      "  Updated bias: 0.461766\n",
      "\n",
      "Epoch 15:\n",
      "  Layer output: 0.967410\n",
      "  Loss: 0.001062\n",
      "  Gradients of w: array([-0.00835821, -0.0125373, 0.00417911], dtype=float32)\n",
      "  Gradient of b: -0.004179\n",
      "  Updated weights: array([-0.20684, 0.818423, 0.447264], dtype=float32)\n",
      "  Updated bias: 0.462184\n",
      "\n",
      "Epoch 16:\n",
      "  Layer output: 0.967810\n",
      "  Loss: 0.001036\n",
      "  Gradients of w: array([-0.0081562, -0.0122343, 0.0040781], dtype=float32)\n",
      "  Gradient of b: -0.004078\n",
      "  Updated weights: array([-0.206025, 0.819646, 0.446856], dtype=float32)\n",
      "  Updated bias: 0.462591\n",
      "\n",
      "Epoch 17:\n",
      "  Layer output: 0.968195\n",
      "  Loss: 0.001012\n",
      "  Gradients of w: array([-0.00796373, -0.0119456, 0.00398187], dtype=float32)\n",
      "  Gradient of b: -0.003982\n",
      "  Updated weights: array([-0.205228, 0.820841, 0.446458], dtype=float32)\n",
      "  Updated bias: 0.462990\n",
      "\n",
      "Epoch 18:\n",
      "  Layer output: 0.968567\n",
      "  Loss: 0.000988\n",
      "  Gradients of w: array([-0.00778014, -0.0116702, 0.00389007], dtype=float32)\n",
      "  Gradient of b: -0.003890\n",
      "  Updated weights: array([-0.20445, 0.822008, 0.446069], dtype=float32)\n",
      "  Updated bias: 0.463379\n",
      "\n",
      "Epoch 19:\n",
      "  Layer output: 0.968926\n",
      "  Loss: 0.000966\n",
      "  Gradients of w: array([-0.00760482, -0.0114072, 0.00380241], dtype=float32)\n",
      "  Gradient of b: -0.003802\n",
      "  Updated weights: array([-0.20369, 0.823149, 0.445688], dtype=float32)\n",
      "  Updated bias: 0.463759\n",
      "\n",
      "Final parameters:\n",
      "Weights: array([-0.20369, 0.823149, 0.445688], dtype=float32)\n",
      "Bias: 0.463759\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "\n",
    "class Neuron(nn.Module):\n",
    "    def __init__(self, nin):\n",
    "        super().__init__()\n",
    "        self.w = mx.random.uniform(low=-1, high=1, shape=(nin,))\n",
    "        self.b = mx.random.uniform(low=-1, high=1, shape=(1,))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        act = mx.sum(self.w * x) + self.b\n",
    "        out = mx.tanh(act)\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.w, self.b]\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super().__init__()\n",
    "        # Creating 'nout' neurons, each with 'nin' inputs\n",
    "        # This structure is similar to PyTorch's nn.Linear layer\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Forward pass of the layer: compute output for each neuron\n",
    "        # This is equivalent to a matrix multiplication in PyTorch\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        # If there's only one output, return it directly instead of a list\n",
    "        # This helps in creating the final layer of the network\n",
    "        return outs[0] if len(outs) == 1 else mx.stack(outs)\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Collecting parameters from all neurons in the layer\n",
    "        # This flattens the list of parameters, similar to PyTorch's approach\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "def mse_loss(pred, target):\n",
    "    return mx.mean((pred - target) ** 2)\n",
    "\n",
    "def forward_and_loss(params, x, target):\n",
    "    layer = Layer(x.shape[0], 1)\n",
    "    layer.neurons[0].w, layer.neurons[0].b = params\n",
    "    pred = layer(x)\n",
    "    loss = mse_loss(pred, target)\n",
    "    return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    layer = Layer(3, 1)  # Layer with 3 inputs and 1 output (single neuron for simplicity)\n",
    "    x = mx.array([2.0, 3.0, -1.0])\n",
    "    target = mx.array([1.0])\n",
    "    \n",
    "    loss_and_grad_fn = mx.value_and_grad(forward_and_loss)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(20):  # 20 epochs for demonstration\n",
    "        output = layer(x)\n",
    "        params = layer.parameters()\n",
    "        loss, grads = loss_and_grad_fn(params, x, target)\n",
    "        \n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        print(f\"  Layer output: {output.item():.6f}\")\n",
    "        print(f\"  Loss: {loss.item():.6f}\")\n",
    "        print(f\"  Gradients of w: {grads[0]}\")\n",
    "        print(f\"  Gradient of b: {grads[1].item():.6f}\")\n",
    "        \n",
    "        # Parameter update (gradient descent)\n",
    "        learning_rate = 0.1\n",
    "        layer.neurons[0].w = layer.neurons[0].w - learning_rate * grads[0]\n",
    "        layer.neurons[0].b = layer.neurons[0].b - learning_rate * grads[1]\n",
    "        \n",
    "        print(f\"  Updated weights: {layer.neurons[0].w}\")\n",
    "        print(f\"  Updated bias: {layer.neurons[0].b.item():.6f}\")\n",
    "        print()\n",
    "\n",
    "    print(\"Final parameters:\")\n",
    "    print(f\"Weights: {layer.neurons[0].w}\")\n",
    "    print(f\"Bias: {layer.neurons[0].b.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3fb69a3-e058-482f-8b00-d8b2139e60bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "  MLP output: 0.999581\n",
      "  Loss: 0.000000\n",
      "Epoch 10:\n",
      "  MLP output: 0.999581\n",
      "  Loss: 0.000000\n",
      "Epoch 20:\n",
      "  MLP output: 0.999581\n",
      "  Loss: 0.000000\n",
      "Epoch 30:\n",
      "  MLP output: 0.999581\n",
      "  Loss: 0.000000\n",
      "Epoch 40:\n",
      "  MLP output: 0.999581\n",
      "  Loss: 0.000000\n",
      "Epoch 50:\n",
      "  MLP output: 0.999581\n",
      "  Loss: 0.000000\n",
      "Epoch 60:\n",
      "  MLP output: 0.999581\n",
      "  Loss: 0.000000\n",
      "Epoch 70:\n",
      "  MLP output: 0.999581\n",
      "  Loss: 0.000000\n",
      "Epoch 80:\n",
      "  MLP output: 0.999581\n",
      "  Loss: 0.000000\n",
      "Epoch 90:\n",
      "  MLP output: 0.999581\n",
      "  Loss: 0.000000\n",
      "\n",
      "Final prediction:\n",
      "MLP output: 0.999581\n",
      "Target: 1.000000\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "\n",
    "class Neuron(nn.Module):\n",
    "    def __init__(self, nin):\n",
    "        super().__init__()\n",
    "        self.w = mx.random.uniform(low=-1, high=1, shape=(nin,))\n",
    "        self.b = mx.random.uniform(low=-1, high=1, shape=(1,))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        act = mx.sum(self.w * x) + self.b\n",
    "        return mx.tanh(act)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.w, self.b]\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super().__init__()\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else mx.stack(outs)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, nin, nouts):\n",
    "        super().__init__()\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "def mse_loss(pred, target):\n",
    "    return mx.mean((pred - target) ** 2)\n",
    "\n",
    "def forward_and_loss(params, mlp, x, target):\n",
    "    # Reconstruct MLP with given parameters\n",
    "    param_idx = 0\n",
    "    for layer in mlp.layers:\n",
    "        for neuron in layer.neurons:\n",
    "            neuron.w, neuron.b = params[param_idx:param_idx+2]\n",
    "            param_idx += 2\n",
    "    \n",
    "    pred = mlp(x)\n",
    "    loss = mse_loss(pred, target)\n",
    "    return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an MLP with 3 inputs, two hidden layers of 4 neurons each, and 1 output\n",
    "    mlp = MLP(3, [4, 4, 1])\n",
    "    x = mx.array([2.0, 3.0, -1.0])\n",
    "    target = mx.array([1.0])\n",
    "    \n",
    "    loss_and_grad_fn = mx.value_and_grad(forward_and_loss)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(100):  # 100 epochs for demonstration\n",
    "        params = mlp.parameters()\n",
    "        loss, grads = loss_and_grad_fn(params, mlp, x, target)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            output = mlp(x)\n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"  MLP output: {output.item():.6f}\")\n",
    "            print(f\"  Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        # Parameter update (gradient descent)\n",
    "        learning_rate = 0.01\n",
    "        param_idx = 0\n",
    "        for layer in mlp.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.w = neuron.w - learning_rate * grads[param_idx]\n",
    "                neuron.b = neuron.b - learning_rate * grads[param_idx + 1]\n",
    "                param_idx += 2\n",
    "\n",
    "    print(\"\\nFinal prediction:\")\n",
    "    print(f\"MLP output: {mlp(x).item():.6f}\")\n",
    "    print(f\"Target: {target.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "605807a5-f815-49a1-8bdc-f1636d4a13da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Structure:\n",
      "Input: 3 neurons\n",
      "Hidden layer 1: 4 neurons\n",
      "Hidden layer 2: 4 neurons\n",
      "Output layer: 1 neuron\n",
      "\n",
      "Input: array([2, 3, -1], dtype=float32)\n",
      "Output: -0.740931\n",
      "\n",
      "Multiple forward passes:\n",
      "Pass 1 output: -0.740931\n",
      "Pass 2 output: -0.740931\n",
      "Pass 3 output: -0.740931\n",
      "Pass 4 output: -0.740931\n",
      "Pass 5 output: -0.740931\n",
      "\n",
      "Different inputs:\n",
      "Input 1 array([1, 2, 3], dtype=float32): Output: -0.328161\n",
      "Input 2 array([-1, 0.5, 2], dtype=float32): Output: -0.531142\n",
      "Input 3 array([0, 0, 0], dtype=float32): Output: -0.748314\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "\n",
    "class Neuron(nn.Module):\n",
    "    def __init__(self, nin):\n",
    "        super().__init__()\n",
    "        self.w = mx.random.uniform(low=-1, high=1, shape=(nin,))\n",
    "        self.b = mx.random.uniform(low=-1, high=1, shape=(1,))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        act = mx.sum(self.w * x) + self.b\n",
    "        return mx.tanh(act)\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super().__init__()\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else mx.stack(outs)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, nin, nouts):\n",
    "        super().__init__()\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Example usage of the MLP\n",
    "if __name__ == \"__main__\":\n",
    "    # Creating a network with 3 inputs, two hidden layers of 4 neurons each, and 1 output\n",
    "    x = mx.array([2.0, 3.0, -1.0])\n",
    "    n = MLP(3, [4, 4, 1])\n",
    "    \n",
    "    # Forward pass\n",
    "    output = n(x)\n",
    "    \n",
    "    print(\"MLP Structure:\")\n",
    "    print(f\"Input: 3 neurons\")\n",
    "    print(f\"Hidden layer 1: 4 neurons\")\n",
    "    print(f\"Hidden layer 2: 4 neurons\")\n",
    "    print(f\"Output layer: 1 neuron\")\n",
    "    print(f\"\\nInput: {x}\")\n",
    "    print(f\"Output: {output.item():.6f}\")\n",
    "\n",
    "    # Demonstrating multiple forward passes\n",
    "    print(\"\\nMultiple forward passes:\")\n",
    "    for i in range(5):\n",
    "        output = n(x)\n",
    "        print(f\"Pass {i+1} output: {output.item():.6f}\")\n",
    "\n",
    "    # Demonstrating with different inputs\n",
    "    print(\"\\nDifferent inputs:\")\n",
    "    inputs = [\n",
    "        mx.array([1.0, 2.0, 3.0]),\n",
    "        mx.array([-1.0, 0.5, 2.0]),\n",
    "        mx.array([0.0, 0.0, 0.0])\n",
    "    ]\n",
    "    for i, inp in enumerate(inputs):\n",
    "        output = n(inp)\n",
    "        print(f\"Input {i+1} {inp}: Output: {output.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41adc9c-2bd4-47f4-a0c5-2e30f127c94b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
