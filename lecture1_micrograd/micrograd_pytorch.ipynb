{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cd40caf-9ecf-45d3-9587-c88d49c5d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9afede2c-baaa-41f3-95bc-d87734d85f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "# Creating tensor objects with requires_grad=True to enable autograd\n",
    "# This mimics the Value class from micrograd, allowing us to track computations\n",
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "\n",
    "# Forward pass: Constructing the computational graph\n",
    "# This is similar to how we built expressions in micrograd\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "# Printing the output value, using .data to access the raw tensor without gradient tracking\n",
    "print(o.data.item())\n",
    "\n",
    "# Backward pass: Computing gradients\n",
    "# This is equivalent to calling .backward() on the loss in micrograd\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "# Printing gradients\n",
    "# In micrograd, we accessed .grad directly on Value objects\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "141adb11-9577-47e8-9e53-cc7ada10ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        # Initializing weights and bias with random values\n",
    "        # This is similar to how PyTorch initializes parameters\n",
    "        # In micrograd, we used Value objects for weights and biases\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Forward pass of a neuron: w * x + b\n",
    "        # This is the same mathematical operation we did in micrograd\n",
    "        # Using sum() with a generator expression for efficiency\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Returning all parameters (weights and bias) of the neuron\n",
    "        # This mimics PyTorch's way of accessing parameters\n",
    "        return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        # Creating 'nout' neurons, each with 'nin' inputs\n",
    "        # This structure is similar to PyTorch's nn.Linear layer\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Forward pass of the layer: compute output for each neuron\n",
    "        # This is equivalent to a matrix multiplication in PyTorch\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        # If there's only one output, return it directly instead of a list\n",
    "        # This helps in creating the final layer of the network\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Collecting parameters from all neurons in the layer\n",
    "        # This flattens the list of parameters, similar to PyTorch's approach\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        # Creating layers based on input size and list of output sizes\n",
    "        # This allows for flexible network architectures\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Forward pass through all layers\n",
    "        # This is similar to PyTorch's Sequential container\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Collecting parameters from all layers\n",
    "        # This mimics PyTorch's way of gathering all parameters for optimization\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7b270e1-759e-4177-b4d5-f247370b5c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9454904521169566)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage of the MLP\n",
    "# Creating a network with 3 inputs, two hidden layers of 4 neurons each, and 1 output\n",
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd13a874-ce11-424c-bd1c-63259d64ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "# This is a simple dataset for binary classification\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]  # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c23d2326-6eb5-41d5-b38f-ace8999deefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.7614611225411547\n",
      "1 3.586164055770185\n",
      "2 3.2288574674146737\n",
      "3 2.391882072245139\n",
      "4 1.4688342308802875\n",
      "5 0.7685236492845273\n",
      "6 0.24036820203407314\n",
      "7 0.07391691362663215\n",
      "8 0.05769338818159821\n",
      "9 0.047098169959730846\n",
      "10 0.03965150176504377\n",
      "11 0.03414733995351973\n",
      "12 0.0299235894113843\n",
      "13 0.026586570098997824\n",
      "14 0.023887862214180766\n",
      "15 0.021663210896747536\n",
      "16 0.0197998162878694\n",
      "17 0.01821774061954757\n",
      "18 0.016858811260829384\n",
      "19 0.015679722233846147\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# This demonstrates the full process of forward pass, loss calculation, backpropagation, and update\n",
    "for k in range(20):\n",
    "    \n",
    "    # Forward pass\n",
    "    # ypred = [n(x) for x in xs]\n",
    "    # This line performs the forward pass of the entire neural network for each input in the dataset\n",
    "    # 1. It iterates over each input 'x' in the training data 'xs'\n",
    "    # 2. For each 'x', it calls the neural network 'n(x)', which:\n",
    "    #    a. Passes the input through each layer of the network\n",
    "    #    b. Applies the weights, biases, and activation functions\n",
    "    #    c. Produces an output prediction\n",
    "    # 3. The result is a list of predictions, one for each input in the dataset\n",
    "    # 4. This step is crucial as it generates the network's current predictions,\n",
    "    #    which will be used to calculate the loss and subsequently update the weights\n",
    "    # In PyTorch, this would typically be done with a single tensor operation for efficiency,\n",
    "    # but here we're using a list comprehension for clarity and to match our custom MLP implementation\n",
    "    ypred = [n(x) for x in xs]\n",
    "    # Computing Mean Squared Error (MSE) loss\n",
    "    # This is a simple loss function for demonstration, similar to what we used in micrograd\n",
    "    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "    \n",
    "    # Backward pass\n",
    "    # Zeroing out gradients before backpropagation\n",
    "    # This is crucial to avoid accumulating gradients from previous iterations\n",
    "    # In micrograd, we did this manually for each Value object\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update step: Simple gradient descent\n",
    "    # This is the same update step we used in micrograd\n",
    "    # Learning rate is set to 0.1\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.1 * p.grad\n",
    "    \n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45cd4f78-d946-4b18-af7c-a7cda2c80389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.970912240517384),\n",
       " Value(data=-0.984132482631298),\n",
       " Value(data=-0.9102155416160583),\n",
       " Value(data=0.9192497838554978)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New values\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43108c0a-80c0-4617-84be-e415839e75bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
